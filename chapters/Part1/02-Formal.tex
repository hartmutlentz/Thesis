\documentclass[openright,twoside,headsepline]{scrbook}
\usepackage[applemac]{inputenc}
\usepackage{graphicx,xcolor,natbib,hyperref} % obsolete in HU-diss
\input{/Users/lentz/Documents/GitHub_locals/Thesis/adjustment}

\begin{document}
\graphicspath{{/Users/lentz/Documents/GitHub_locals/Thesis/images/}}
\tableofcontents



\chapter{Theory}
This chapter is devoted to the mathematical formalism that is used to model infectious diseases and networks.
We define a mathematical framework and summarize relevant results of earlier research in this chapter.
In addition, section xx describes an efficient computer implementation of networks.

\section{Models of infectious diseases}
%\subsection{Very briefly:}
\paragraph{Main observations\color{Cayenne}{.}} Large scale patterns of epidemics have been measured \cite{giehl:2012}.
The spread of infectious diseases is something that everyone is familiar with.

\paragraph{Research field: Epidemiology\color{Cayenne}{.}} One goal of epidemiology is to understand the principles behind the spreading process, i.e. the way how a disease is transmitted through a population.
In this context, \emph{conceptional} models are used.
They make use of simple assumptions for the local (person-to-person) dynamics and focus on the big picture of the process.
Conceptual models are very similar to models in theoretical physics, because they focus on the very essence of the problem (here: the macroscopic view, spreading patterns).
However, they have to neglect many details of the real problem (here: physiology, symptoms, individual behavior, infection pathways and many more!) in order to have mathematical feasible models.

Another important issue of epidemiology is the \emph{forecast} of epidemic spreading processes.
Forecast models incorporate as much information as possible and the main focus is not an understanding of the basic principles.

This section summarizes the mathematical framework that roughly reproduces the behavior of infectious diseases and briefly discusses some major insights.


\subsection{Development of mathematical epidemiology}
The modeling of infection diseases mostly uses the concept of compartment models as explained in section xx.
Major contributions to the modern theoretical framework were provided by \cite{kermack:27}, \cite{bailey:57} and \cite{andersonmay:92}.
In his review about the mathematics of infectious diseases Hethcote reports a model for smallpox was already formulated in 1760 by D. Bernoulli (\cite{Hethcote:2000} and references therein).
In the early 20th century, people developed mathematical models for epidemics: a discrete time model in 1906 \cite{Hamer} and a differential equation model in 1911 \cite{Ross}.
The epidemic threshold (section \ref{sec:sir_model}) was found in the 1920s \cite{kermack:27}.
Starting from Bailey's book \cite{bailey:57} in the 1950s, the modeling of infectious diseases became a major scientific research field.
Modern models of infectious diseases include vaccination, demographic structure, disease vectors, quarantine and even game theory (\cite{Bauch:2004} and references in \cite{Hethcote:2000}).
The availability of contact data in recent years led to a strong impact on network analysis on epidemiology.
Well known concepts of mathematics (graph theory \cite{Bollobas:1985}) and social sciences (social network analysis \cite{WassermanFaust}) have been adopted to disease modeling, since the connections between individuals are related to their epidemic spreading potential \cite{Keeling:2005}.

\subsection{Infection dynamics}

The spread of infectious diseases can be modeled in terms of compartment models as described in section xx.
We differentiate between \emph{conceptional models} and \emph{realistic disease models}.
While the former class is used to provide conceptual results as for the computation of thresholds or to test theories \cite{Hethcote:2000}, realistic disease models use as many aspects as possible to provide a forecast of the spreading process.
Realistic disease models can be very complex and are beyond the scope of this work, thus we focus on the use of conceptional models.
The following section is inspired by the Lecture notes of J.~R.~Chasnov \cite{Chasnov:2010}.

\subsection{SI model}\label{sec:si_model}
Let us consider a population of $N$ individuals.
In the simplest case, the infection status of each individual is either susceptible or infected and there are no births and deaths on the population.
Susceptible individuals become infected, if they are in contact with an infected.
This mimics the behavior of an infectious disease without immunization, i.e. infected individuals stay permanently infected.

Provided that $\alpha $ is the rate, under which new susceptible become infected, the SI-model is as follows:
\begin{align}\label{eq:si_model}
\frac{dS}{dt} &= -\alpha SI \nonumber \\
\frac{dI}{dt} &= \alpha SI,
\end{align}
where $S$ and $I$ are the numbers of susceptible and infected individuals respectively.
The total population is $N=S+I$.
Thus, \eqref{eq:si_model} can be rewritten as
\[
\frac{dI}{dt}=\alpha (N-I)I,
\]
i.e. a logistic differential equation.
Hence, in the limit $t\rightarrow \infty $ the whole population is infected ($I(\infty )=N$). 

\subsection{SIR model}\label{sec:sir_model}
In contrast of the infection dynamics introduced in the previous section, many epidemics allow for an immunization of the individuals.
Examples are measles or whooping cough \cite{grenfell:92} \cite{andersonmay:92}.
In this case, individuals recover from the disease after being infected for a certain time period.
The infection scheme has to be extended to susceptible-infected-recovered (SIR) as in the following infection model \cite{kermack:27}:
\begin{align}\label{eq:sir_model}
\frac{dS}{dt} &= -\alpha SI \nonumber \\
\frac{dI}{dt} &= \alpha SI -\gamma I \nonumber \\
\frac{dR}{dt} &= \gamma I
\end{align}
where $\alpha $ is the infection rate and $\gamma $ is the immunization or recovery rate.
There is no analytic solution for the system \eqref{eq:sir_model}, but some fundamental conclusions can be obtained analytically.
We show a typical solution of \eqref{eq:sir_model} in figure \ref{fig:std_sir_model}.
%
\begin{SCfigure}%[htbp]
%\begin{center}
\includegraphics{sir_model.pdf}
\caption{Solution of the susceptible-infected-recovered (SIR) model \eqref{eq:sir_model}.
The number of infected shows that the spreading process is a single event.
Note that a fraction of the population is still susceptible at the end of the process.
Parameters: $\alpha = 3$, $\gamma = 1$, $N=300$, $S_0=1$.}
\label{fig:std_sir_model}
%\end{center}
\end{SCfigure}
%


The SIR model shows more sophisticated features than the SI model \eqref{eq:si_model}.
To begin with, we analyze the fixed points of the system, i.e. $(S_*,I_*,R_*)$ where
\begin{equation}
\frac{dS_*}{dt} = -\alpha S_*I_* =0 ,\; \;\;
\frac{dI_*}{dt} = \alpha S_*I_* -\gamma I_* =0,\; \;\;
\frac{dR_*}{dt} = \gamma I_* = 0.
\end{equation}
It follows from the last equation that $I_*=0$ at the fixed point, where $S_*$ and $R_*$ can be arbitrary.
Hence, a fixed point is $(S_*,0,R_*)$.

Let us analyze the stability of the fixed point in the early phase of an infection.
Almost all individuals are susceptible and consequently $I_*=N-S_*$.
An outbreak occurs, if and only if $dI/dt >0$ in this phase, i.e.
\begin{equation}\label{eq:prelim_condition}
\frac{dI}{dt}=\alpha S_* (N-S_*) - \gamma (N-S_*)=(N-S_*)(\alpha S_* -\gamma ) >0.
\end{equation}
It follows from \eqref{eq:prelim_condition} that the number of infected grows, if
\begin{equation}\label{eq:prelim_rnod}
\alpha S_* / \gamma >1.
\end{equation}
Equation \eqref{eq:prelim_rnod} is extremely important in epidemiology, because it defines a threshold for the unfolding of an infection spreading process.
We call this fraction the \emph{basic reproduction number} $R_0$.
Recall that $S_* \approx N$ in the fixed point.
Thus it follows that the outbreak condition is
\begin{equation} \label{eq:r0}
R_0 = N \frac{\alpha }{\gamma } >1.
\end{equation}

The basic reproduction number describes the average number of follow-up infections by each infected individual.
It is one of the main goals in epidemiology to bring down the basic reproduction number of a disease below the critical value $R_0=1$.
This is the reason for the implementation of mass vaccination.
As one can immediately see from equation \eqref{eq:r0}, this can be done by reducing the infection rate $\alpha $ or by increasing the immunization rate $\gamma $.
In principle, one could also reduce the size of the initial population $S_*$.
As an example, reducing the infection rate can be done by increasing hygiene standards or appropriate behavior, say wearing warm clothes in winter time to avoid common cold.
The immunization rate can be increased by vaccination.

Let us now focus on the late phase of an SIR-infection.
In contrast to the SI-model of section \ref{sec:si_model} an SIR like outbreak does not necessarily infect the whole population, even if $R_0>1$.
The reason is that there has to be a critical mass of susceptible individuals in order to keep an infection alive (see equation \eqref{eq:prelim_rnod}).
The total number of infected during an infection given by the number of recovered at the end of the infection, since every recovered has to be in the infected state in the first place.
A central measure throughout this work is therefore the \emph{outbreak size} $R_\infty$.

To compute the outbreak size, we consider the second fixed point of \eqref{eq:sir_model}, i.e. the fixed point for $t \rightarrow \infty $.
At this point there are no infected and a fraction of the population is recovered.
Hence the fixed point is $(N-R_\infty , 0, R_\infty )$.
A simple way to obtain the outbreak size $R_\infty $ is to use equations \eqref{eq:sir_model} and compute 
\[
\frac{dS}{dR}=-\frac{\alpha }{\gamma } S 
\]
and separate the variables \cite{Chasnov:2010}.
This yields
\[
\int _{S_*} ^{N-R_\infty} \frac{dS}{S}=-\frac{\alpha }{\gamma } \int _{R_*} ^{R_\infty} dR .
\]
We integrate from the initial condition at $t=0$ to the final condition at $t \rightarrow \infty$, where $S_\infty = N-R_\infty $.
Using that $R_* =0$ at $t=0$ gives 
\begin{equation}\label{eq:transcendental}
R_\infty = S_*-S_* e ^{-\frac{\alpha}{\gamma}R_\infty}.
\end{equation}
This transcendental equation can be solved numerically using a Newton-Raphson technique.
The outbreak size $R_\infty $ only takes finite values for $\alpha / \gamma > 1$.
A solution of equation \eqref{eq:transcendental} is shown in figure \ref{fig:transcendental}
%
\begin{figure}[htbp]
\begin{center}
\includegraphics{R_0_R_infty.pdf}
\caption{Relative outbreak size vs. basic reproduction number.
The outbreak size takes finite values only for $R_0/N >1$.
Note that even for supercritical $R_0$ the outbreak size is in general smaller than the total population.
}
\label{fig:transcendental}
\end{center}
\end{figure}

It should be noted that an SIR epidemic is a single event, i.e. it possesses a \emph{characteristic time scale}.
The analysis of the late phase of an epidemic also gives information about these time scales.
Let us consider the second equation of \eqref{eq:sir_model}.
\begin{equation}\label{eq:sir_model_only_i}
\frac{dI}{dt} = \alpha SI -\gamma I
\end{equation}
In the late phase of an SIR-type epidemic, the fraction of infected is small.
Given sufficiently large values of $R_0$, the fraction of recovered is also small in this phase (see figure \ref{fig:transcendental}).
Thus, we neglect the quadratic term in \eqref{eq:sir_model_only_i}.
This gives $\frac{dI}{dt} = -\gamma I $, which has the solution
\[
I(t)=I(0)e^{-\gamma t}.
\]
Hence, the infection decays exponentially for large $t$ and the inverse recovery rate $1/\gamma $ defines the characteristic time of the epidemic.

\subsection{Force of infection}
The model presented in section \ref{sec:sir_model} describes only the very basic behavior of epidemic dynamics, and is therefore a conceptual model.
However, it is one of the main objectives in epidemiology to have an understanding of the exact infection rates in the process.
Infection rates their selves can cause complex infection dynamics.

The term $\alpha I$ used in section \ref{sec:sir_model} is a special, very simple case of an infection rate.
More generally, we have to replace $\alpha I$ by an abstract infection rate $\lambda $ containing more information about the interaction between susceptible and infected individuals \cite{Keeling:2005}.
Thus, the equation for the infected becomes
\[
dI/dt = -\lambda S -\gamma I.
\]
The rate $\lambda $ is called the \emph{force of infection}.
In principle, this parameter can be arbitrarily complex, because it contains detailed information about the mixing properties of the population.
This information could be given as contact networks, demographic contact structures, etc.

In most cases, detailed information about mixing is not available.
Instead, we assume \emph{random mixing} of the population, i.e. every individual can be in contact with every other individual.
This yields a transmission rate \cite{Keeling:2005}
\begin{equation}\label{eq:force_of_infection}
\lambda = \tau n \frac{I}{N}\equiv \beta \frac{I}{N},
\end{equation}
where $\tau $ is the transmission rate, $n$ is the effective contact rate and $I/N$ is the fraction of infectious contacts.
It is therefore reasonable to replace the infection term $\alpha $ in \eqref{eq:sir_model} by $\beta /N$ to explicitly include the force of infection.
Nevertheless, the results presented in section \ref{sec:sir_model} remain qualitatively the same.

Although the force of infection gives a more reasonable description of the infection process, the assumption of random mixing remains inappropriate for many real world systems.
Due to the availability of contact data, the random mixing assumption can be improved in terms of contact networks.
Even if the exact data of an epidemic system is not available, research on complex networks allows us to give more realistic models about mixing.
In the next section, we briefly report important results in complex network research and focus on the interplay between networks and epidemics in section xx.


\section{Network theory}\label{sec:network_theory}
As we have seen in the previous section, standard epidemic models make use of the random mixing assumption.
This assumption seems reasonable, if no further information about the contact structure within a population is available, because it gives a worst case scenario of the infection dynamics.
Even an overestimation of the outbreak size can be corrected by introducing smaller, effective disease parameters.
However, the random mixing assumption does not allow for non homogenous mixing, i.e. each individual is considered equal.
Nevertheless, the equality of individuals is not a reasonable assumption for many epidemic substrates.
Examples are contact structures of humans, livestock trade, vehicles as disease vectors of links between computers.

\paragraph{Main observations\color{Cayenne}{.}}
The random mixing assumption is obsolete in the vast majority of systems.
Instead, these systems possess an underlying contact structure -- a network.
Since the beginning of the 21st century, large amounts of data about these contact structures became available for social, economic, transportation and biological networks.
Observations showed that many real-world networks share common topological properties (see section \ref{sec:macro_measures}).
However, the number of their non-trivial topological properties is considerable, therefore they are often referred to as \emph{complex} networks.

\paragraph{Research field: network science\color{Cayenne}{.}}
Modern network science is an interdisciplinary research field, because it addresses systems of diverse scientific affinity.
Its roots lie in graph theory (mathematics) and social network analysis (social sciences).
Social network analysis plays a particular role for the definition of local network measures (see section \ref{sec:micro_measures}), whereas the influence of graph theory is stronger in macroscopic problems like percolation or graph partitioning.
An important focus of network science is to find common features of different networks and to find the basic principles behind their emergence.
Applied network science is often found in computer science.

\subsection{Matrix representations}\label{sec:network_matrices}
A network is a system consisting of nodes that are connected by edges.
Edges can be undirected, directed and weighted.
In principle, a network can consist of edges of different types.
This can be represented by multiple networks sharing the same set of nodes, but different edges.

Networks are called graphs in mathematical literature.
A graph $G=(V,E)$ is a set of nodes (or vertices) $V$ and edges (or arcs) $E$, where each edge is given by the tuple of nodes it connects, i.e. $e_1 =(u,v) \in E$ connects nodes $u$ and $v$.
An edge $(u,v)$ being present in an undirected network implies an edge $(v,u)$.
Apparently, this does not hold in directed networks.
Edges of weighted networks carry additional meta information about their weight.
This meta information can be their importance, capacity, number of transported items or the geographical distance between nodes $u$ and $v$.

Graphs can be represented by different graph matrices, where different matrix representation emphasize typical properties of the network.
The most common graph matrix is the \emph{adjacency matrix} $\mathbf{A}$ with entries
\begin{equation}\label{eq:adjacency_matrix}
a_{ij}\equiv (\mathbf{A})_{ij}= 
\begin{cases}
1 & \text{if $i$ is connected to $j$} \\
0 & \text{else.}
\end{cases} 
\end{equation}
An adjacency matrix contains the edges of the graph and can be seen of the most fundamental graph representation.
Figure \ref{fig:simple_digraph} shows a simple example of a directed graph and its adjacency matrix.
The corresponding matrix would be symmetric in the undirected case.
Weighted networks can be represented by weight matrices, where the values of the entries in \eqref{eq:adjacency_matrix} are not restricted to zero and one.
%
\begin{SCfigure}%[h]
%\begin{center}
\includegraphics{Simple_DiGraph}
\caption[Adjacency matrix.]{A simple directed network. The corresponding adjacency matrix is
\[
\mathbf{A}=\left(\begin{array}{cccc}0 & 1 & 1 & 0 \\0 & 0 & 0 & 1 \\0 & 1 & 0 & 0 \\0 & 1 & 1 & 0\end{array}\right).
\]
}
\label{fig:simple_digraph}
%\end{center}
\end{SCfigure}

The adjacency matrix of an undirected network is symmetric, because every non-zero entry $a_{ij}=1$ implies an edge into the opposite direction, $a_{ji}=1$.
Entries on the main diagonal $a_{ii}$ correspond to nodes with self loops, i.e. nodes with edges pointing back to themselves.
The $i$-th row the adjacency matrix contains non-zero entries $a_{ij}=1$, whenever node $i$ is connected to node $j$.
Hence, every row can be interpreted as the neighborhood of one node.
This holds for undirected and for directed networks.
The columns of $\mathbf{A}$ give the same information as the rows.
In the directed case, however, rows contain the out-neighborhood of each node and columns contain the in-neighborhood, respectively.

Information about paths of a certain length can be obtained using the powers of the adjacency matrix.
The adjacency matrix gives information about the number of paths of length $1$ between node pairs.
Evidently, the number of paths of length $2$ between two nodes $i$ and $j$ is given by $(\mathbf{A}^2)_{ij}$.
This applies also to paths of arbitrary length $n$ using the elements of $\mathbf{A}^n$.

An important example for weighted network matrices is a \emph{Markov chain}.
A Markov chain is a random process without memory and with discrete state space and discrete time.
It is called time-homogenous, if the transition rates are constant.
Time-homogenous Markov chains can be represented as weighted networks and the corresponding weighted adjacency matrix is the \emph{transition matrix}.
Transition matrices are stochastic matrices, i.e. the elements of every row sum up to unity.
Each node represents a different state of the system and the edges are weighted with the probabilities to transition into other states adjacent to these edges.
It is obvious that a transition matrix representation is useful to describe random walks on networks.
An example of such a process is shown in figure \ref{fig:markov_chain}.
The underlying network represents a line of locations, where the drunkard can be located.
At every time-step there is a certain probability to move to another location.
%
\begin{SCfigure}%[htbp]
%\begin{center}
\includegraphics{Markov_chain.pdf}
\caption{Trajectory of a toddling drunk man as an example of a Markov chain.
At every location there is a probability for the drunkard to go left or right.
The node rightmost node is an absorbing state and could model a park bench.
Weights at arrowheads mark the transition probability.
(inspired by \cite{Aldous_book}).}
\label{fig:markov_chain}
%\end{center}
\end{SCfigure}
%
The state of the random walker can be described by a probability vector $\mathbf{p}$, where the initial state of figure \ref{fig:markov_chain} is $\mathbf{p}=(0,1,0,0)$.
The transition matrix $\mathbf{M}$ is a weighted adjacency matrix as it follows from the figure.
Given a state $\mathbf{p}_{t}$ at time $t$, the state of the next time step is given by $\mathbf{p}_{t+1}=\mathbf{p}_t \mathbf{M}^T$.
The equilibrium state $\mathbf{p}_\mathrm{eq}$ follows in the limit $\lim _{n\rightarrow \infty } \mathbf{p}_0 (\mathbf{M}^T)^n$, i.e. the equilibrium state is given by the dominant eigenvector of $\mathbf{M}$.

As a special case of transition matrices, the author would like to name the \emph{Google matrix}.
It describes a random walk on a network, but allows for shortcuts to any node in the network with a certain probability.
The eigenvectors of Google matrices are used for the computation of node rankings according to the PageRank-Algorithm \cite{PageRank}.

Finally, the \emph{Laplace-matrix} of a network is an appropriate representation to model diffusion processes.
For undirected networks the Laplace-matrix is defined as
\begin{equation}\label{eq:laplace_matrix}
\mathcal{L}=\mathbf{D}-\mathbf{A},
\end{equation}
where $\mathbf{A}$ is the adjacency matrix and $\mathbf{D}$ is a diagonal matrix containing the degree $d_i=\sum _j a_{ij}$ of each node.
The definition \eqref{eq:laplace_matrix} has strong analogies to the discrete Laplace-Operator \cite{Press:1992}.
Consequently, they can be used to model diffusion processes on graphs in analogy to Laplace operators in continuous systems (see section xx).

The spectra of adjacency and Laplace matrices contain information about the evolution/history of networks \cite{Banerjee2009}.


\subsection{Network measures}
Before we address ourselves to models of real world networks, we have to introduce methods to measure structural properties of networks.
On the micro scale, this can be done in terms of \emph{node centrality} measures.
These measures are very important to assess the importance of single nodes in the network.
On the macroscopic side, we are interested in the large-scale properties of networks, i.e. percolation, distributions of centralities, connected components or other large scale structures.

\subsubsection{Network terminology}
Let $G=(V,E)$ be a graph consisting of a set of nodes $V$ and a set of edges $E$.
Every route across a graph along its edges without repeating nodes is called a \emph{path}.
Each path is given by an ordered set of the nodes traversed, i.e. $(v_1,v_2,\dots ,v_l)$, with $v_i \in V$ and all edges are in $E$, $v_i,v_{i+1} \subseteq E$.
If there is a path from every node in the network to any other node, the network is called \emph{connected}.
In directed networks, we have to consider two types of connectedness.
A directed network is strongly connected, if there is a directed path between all node pairs and weakly connected, if the node pairs would be connected ignoring the direction of edges.

The \emph{distance} between two nodes is the length of the shortest path between them and the longest distance is the \emph{diameter} of the network.
Every closed path is called a \emph{cycle}.
Graphs that do not contain cycles are called \emph{trees}.
The neighborhood of a node $u$ is the set of all nodes adjacent to it and the size of the neighborhood is the \emph{degree} of the node.
Hence, a node $v$ is in the neighborhood of $u$, if $(u,v) \in E$.
We distinguish between in-degree and out-degree in directed networks.
Finally, $G_0=(V_0,E_0)$ is a \emph{subgraph} of $G=(V,E)$, if $V_0 \subseteq V$ and $E_0 \subseteq E$.


\subsubsection{Microscopic measures}\label{sec:micro_measures}
Given a network, an important question is, if some nodes are more important as other nodes.
Therefore, we summarize measures of the \emph{centrality} of nodes.
The idea if centrality mainly goes back to social network analysis \cite{WassermanFaust,Freeman}, but has been widely adopted and extended in network science.
I restrict myself to those measures, that are indispensable when describing networks.
A more exhaustive overview of centrality measures is found in the review article \cite{MartinezLopez2009} or in online documentations of network analysis software, e.g.~\cite{hagberg2008,networkx}.
In the following, $N$ denotes the order of the network (the number of nodes) and $m$ the number of edges.
%
\begin{figure}[htbp]
\begin{center}
\includegraphics{DiGraph-Centrality}
\caption{A directed network for the demonstration of different centrality measures.}
\label{fig:example_net}
\end{center}
\end{figure}
%

\paragraph{Degree\color{Cayenne}{.}}
The simplest centrality measure is the degree $k$ of a node, which is the number of its neighbors.
In directed network, we distinguish between in-degree $k^-$ and out-degree $k^+$.
The degree follows immediately from the adjacency matrix, i.e.
\[
k ^-(i) = \sum _j a_{ji} \quad \text{and} \quad k ^+ (i)= \sum _j a_{ij}
\]
is the in- and out-degree of node $i$, respectively.
As an example, node $8$ in figure \ref{fig:example_net} has $k ^+(8)=4$ and $k ^- (8)=1$.
In weighted networks, the degree is computed in the same way and is called in-weight and out-weight of a node.

The degree centrality is used in a huge variety of applications.
One of its most important applications is to measure the heterogeneity of network connections, i.e. the existence of hubs in the network.
Hubs are nodes with a degree much larger than the rest of the system.
The heterogeneity of networks can be measured in terms of degree distributions (see section xx).


\paragraph{Closeness\color{Cayenne}{.}} 
The closeness of a node is the reciprocal average distance to all other nodes in the network.
It can be normalized, so that the closeness is $1$, if all other nodes are reachable within one step and $0$ in the limit of infinite distances to all other nodes.
The closeness of a node $i$ in a network of order $N$ is defined as follows:
\begin{equation}\label{eq:closeness}
c(i)=N-1 \sum _j \frac{1}{ d_{ij}}
\end{equation}
where $d_{ij}$ is the distance between nodes $i$ and $j$.
Some tools for an efficient computation of shortest-path distances are introduced in section \ref{sec:implementation}.
It should be noted that the distance between two nodes is defined to be infinite, if the underlying network is not connected.
In this case, the corresponding terms $1/\infty $ do not make contributions in equation \eqref{eq:closeness}.

The closeness centrality is capable to identify nodes with short average pathways to other parts of the network.
Identifying high closeness nodes is therefore reasonable for network navigation.
This holds in particular, if the exact route to the destination is unknown, because nodes with high closeness are probable to reach many destinations quickly.
In \cite{Zweig:closeness} it was shown that nodes of high closeness can act as landmarks for navigation.


\paragraph{Betweenness\color{Cayenne}{.}}
In order to identify nodes that act as bridges between two subgraphs, the measure of betweenness was developed.
In figure \ref{fig:example_net}, node $4$ plays such a role.
It is characteristic for these nodes to contain a relatively large number of shortest paths that have to cross them.
Therefore, betweenness of a node $i$ is defined as
\begin{equation}\label{eq:betweenness}
b(i)=\sum _{s\neq i \neq t} \frac{\sigma _{st}(i)}{\sigma _{st}}
\end{equation}
where $\sigma _{st}$ is the number of shortest paths between nodes $s$ and $t$ and $\sigma _{st} (i)$ is the number of shortest paths between $s$ and $t$ going through node $i$.
The computation of betweenness is expensive using equation \eqref{eq:betweenness}.
Therefore, an efficient algorithm was introduced by Brandes \cite{Brandes:2001p2757}.

Note that bridge nodes might look inconspicuous in the first place, e.g. they could have only two links.
Removing node $5$ in figure \ref{fig:example_net}, for instance, would divide the network into two disjoint subgraphs with nodes $V_1=(1,2,3)$ and $V_2=(5,6,7,8,9)$ respectively.
Therefore, removing nodes of high betweenness from the network has been proven useful in order to divide networks into smaller components \cite{girvan2002,Newman:2004}.


\paragraph{Eigenvector centrality\color{Cayenne}{.}}
The idea of eigenvector centrality can be easily realized by considering Markov chains as in section \ref{sec:network_matrices}.
Frequent multiplication of the transition matrix $\mathbf{M}$ with a random vector gives the largest eigenvector of $\mathbf{M}$.
This relation is known as power method or van Mises iteration \cite{van_mises}.
The dominant eigenvector of the transition matrix gives the equilibrium state of the system.
Using this state as a measure of centrality assigns every node with the probability to find a random walker here after a long period.
The principle behind the dominant eigenvector of an adjacency matrix $\mathbf{A}$ is that important nodes are likely to be connected to other important nodes.
This recursive concept is reflected in the equation
\[
x_i =\frac{1}{\lambda } \sum _j a_{ij} x_j ,
\]
where $x_i$ is the centrality of $i$, $\sum _j a_{ij} x_j$ is the centrality of the neighborhood of $i$ and $\lambda $ is a constant.
This equation can be rewritten as
\begin{equation}
\mathbf{Ax}=\lambda \mathbf{x}.
\end{equation}
It follows from the Perron-Frobenius-Theorem that $\lambda $ must be the largest eigenvalue of $\mathbf{A}$ in order to guarantee all entries of $\mathbf{x}$ to be positive \cite{Bonacich:1972,Bonacich:2007}.
The theorem guaranties unique solutions only to adjacency matrices of connected networks.
Hence, eigenvector centrality is only defined for connected graphs.
Nevertheless, the eigenvector centrality can be computed for each component separately, if a graph is not connected \cite{Bonacich:2007}.

Some important variants of eigenvector centrality are the PageRank and HITS algorithm \cite{Kleinberg:1999,PageRank}.

\paragraph{Node components and range\color{Cayenne}{.}}
The component of a node is the set of nodes it is connected to by a path of any finite length.
We call the size of this set the \emph{range} of a node \cite{Lentz:2012pre}.
In directed networks, we distinguish between the out-component and in-component of a node.
The size of the former is its range and the size of the latter is its reachability.

Apparently, the range of a node is of major importance for any epidemiological problem on a network, because it defines an upper bound for the size of any outbreak starting at this very node. 
Although the range measure is rather simple, it can show an interesting distribution.
The shape of its distribution is inherently related to percolation properties of the network (see section xx).

\subsubsection{Macroscopic measures}\label{sec:macro_measures}
In order to get the big picture about a network, we discuss measures that capture large scale properties of networks.
The central question for the analysis of real-world networks is, whether different networks share similar large-scale features or whether each network is unique.
Is network=network\textbf{\textsf{\color{Cayenne}{?}}}

\paragraph{Degree Distribution\color{Cayenne}{.}}
In principle, the distribution of any centrality measure could yield insights into the macroscopic network structure.
As a matter of fact, the distribution of a networks degrees became a major criterium for the classification into different network types.
If all nodes of a graph have the same degree, the graph is called \emph{regular}.
Lattices are special cases of regular graphs.
In this case, the degree distribution collapses to a single peak without statistical variation.

Observations of real-world networks have shown that some networks exhibit exponential decaying degree distributions, i.e. there is a a variance of degrees, but the system possesses a \emph{typical degree}.
Examples are social networks and technological and economic networks, such as electric power-grids and traffic networks \cite{Amaral:2000,indian_railway}.

The nodes of the vast majority of large real-world networks, however, show a degree variation over several orders of magnitude.
Examples are the network of internet routers \cite{Faloutsos:1999}, www links \cite{Barabasi99} or scientific citations \cite{Price:1965}.
Their degree distributions are approximated by \emph{power-laws} of the form
\begin{equation}\label{eq:scale-free_distr}
P(k) \propto k^{-\gamma } ,
\end{equation}
where $2<\gamma <3$ for most observed networks \cite{all_scale_free_are_sparse,Newman2003}.
The approximation is reasonable for the tails of the distributions, i.e. for large values of $k$.
The identification of power-law distributions in data is discussed in \cite{Clauset:2009}.

Distributions of the form \eqref{eq:scale-free_distr} are called \emph{scale-free}, because they do not show a typical value (mean).
Instead, the network has nodes with only a few neighbors and hubs with very large degrees.
The structural difference between random and scale-free networks is sketched in figure \ref{fig:random_scalefree}.
%
\begin{figure}[htbp]
\begin{center}
\includegraphics{exponential_scalefree}
\caption{Structural difference between networks with exponential and scale-free degree distributions.
All nodes have a similar degree in the random network, while the scale-free network shows hubs with a significantly larger degree than the average.
Hubs are highlighted in red.}
\label{fig:random_scalefree}
\end{center}
\end{figure}

Scale-free networks have attained remarkable attention in the last years and many real-world networks have been conjectured as scale-free \cite{Newman2003,Barabasi99}.
Important consequences of this classification were found to be a change in the threshold behavior of epidemic processes \cite{PastorSatorras:2001} and their topological resilience to node failures \cite{Albert:2000}.
The degree distributions of collaboration networks were well fitted by a scale-free distribution with a sharp cut-off \cite{Newman:2001p,Albert:2000p}, i.e. $P(K)\propto k^{-\gamma }e^{-k/\kappa }$ with fitting constants $\gamma $ and $\tau $.
In \cite{Amaral:2000}, a possible explanation for the existence of an exponential cut-off was the aging of nodes, indicating that real systems possess a natural upper bound for their number of links.



\paragraph{Clustering coefficient\color{Cayenne}{.}}
The idea of the clustering coefficient comes from social networks.
It measures, whether a network contains a significantly large number of triangles.
This behavior is conjectured to be typical for social networks and has the simple meaning:
``a friend of your friend is likely to be your friend''.
The clustering coefficient $C$ is the number of connected triples ($A\rightarrow B \rightarrow C \rightarrow A$) divided by the actual number of triples ($A\rightarrow B \rightarrow C $) in the network.
Using the adjacency matrix $\mathbf{A}$, the clustering coefficient can be computed as follows:
\begin{equation}\label{eq:clustering_coefficient}
C=\frac{\operatorname{tr}(\mathbf{A}^3)}{\operatorname{sum} (A^2) -\operatorname{tr}(\mathbf{A}^2)},
\end{equation}
where $\operatorname{tr}(\mathbf{A} )$ denotes the trace of $\mathbf{A}$ and $\operatorname{sum} (\mathbf{A})=\sum _{ij} a_{ij}$ is the sum over all elements of $\mathbf{A}$.
In this work, we focus on the clustering coefficient as a macroscopic property of networks.
It should be noted that there is also a node clustering coefficient defined by $c_i=\sum _{jl} a_{ij}a_{jl}a_li/(k_i (k_i -1))$ \cite{Watts:1998,dynamical_processes}.
Thus, a network clustering coefficient can also be defined by the average $\mean{c_i}$, which however gives slightly different values as \eqref{eq:clustering_coefficient} and should not be mixed up with the latter.


The clustering coefficient plays an essential role in the small-world model of networks (\cite{Watts:1998}, section \ref{sec:network_models}) and has been found to be an important property not only in social networks \cite{Holland1971}, but in many real-world networks \cite{Newman2003}.

\paragraph{Average shortest path length\color{Cayenne}{.}}
The distance matrix $d_{ij}$ contains the distance between nodes $i$ and $j$ in the network.
Ignoring those node pairs with infinite distance (i.e. setting $d_{ij}=0$) gives the average shortest path length
\begin{equation}\label{eq:}
l=\frac{1}{N(N-1)}\sum _{i, j} d_{ij}
\end{equation}

It is a common feature of many networks that the average shortest path length is much smaller than the number of nodes in the network, i.e. typically networks contain shortcuts \cite{RevModPhys.74}.
An early and impressive example was shown by Milgrim, where the average distance between two randomly chosen people in the united states was measured to be 6 \cite{Milgram:1967}. 
This property is called \emph{small world} phenomenon.
It is an important building block of the Watts-Strogatz network model (\cite{Watts:1998}, section \ref{sec:watts_strogatz_model}).



\paragraph{Connected components\color{Cayenne}{.}}
A connected component $G_\mathrm{cc}=(V_\mathrm{cc},E_\mathrm{cc})$ is a subgraph of graph $G=(V,E)$, where there is a path between any node pair in $V_\mathrm{cc}$.
In directed graphs, connected component in the sense above is called \emph{strongly connected}.
A component is called \emph{weakly connected}, if it is connected ignoring the direction of edges.
Many real-world networks contain one largest connected component that is typically much larger than all other components of the system.
This component is therefore called \emph{giant component}.

In fact, the emergence of a giant component in a network is a 2nd order phase transition and is a graph theoretical percolation process \cite{Newman2003}. 
Components play an important role for epidemic processes, because the component membership of each node defines the maximum outbreak size of any epidemic started at this very node.
In the directed case, maximum outbreak sizes are bounded by the underlying strongly connected component (lower bound) and the out component of the starting node (higher bound).
The general component structure of directed networks is discussed in \cite{Dorogovtsev:2001jd} and we provide further discussion of their epidemiological relevance in section xx.


\paragraph{Accessibility\color{Cayenne}{.}}
If we directly connect each node of a network with all other nodes it is connected to by a path of whatever length, we get the \emph{accessibility} of the network.
Accessibility measures the ability of each node to reach destinations, which is in particular important for transportation systems \cite{Garrison:1960up}.
Mathematically, we define the accessibility graph (also \emph{transitive closure}) of a network as follows:
Let $G=(V,E)$ be a network.
Than $G^*=(V,E^*)$ is the accessibility graph of $G$ with $(u,v) \in E^*$, if there is a path from $u$ to $v$.
The accessibility graph is typically dense, because it contains many more edges than the underlying network.
In mathematical literature accessibility is called \emph{transitive closure} of a network.
A (weighted) adjacency matrix $\mathbf{C}$ of $G^*$ for a $N$-node network is given by the cumulative matrix
\begin{equation}\label{eq:cumulative_matrix}
\mathbf{C}= \sum _{i=1} ^{N-1} \mathbf{A}^i,
\end{equation}
where $\mathbf{A}$ is the adjacency matrix of $G$ and the elements of $\mathbf{C}$ contain the actual number of paths between each node pair.

\subsection{Implementation}\label{sec:implementation}
In order to efficiently implement networks and their analysis on a computer, it is necessary to use data structures adjusted to these problems.
A short and transparent introduction to data structures and algorithms is in the book of Skiena \cite{algorithm_design}.
In this section, we discuss some essential data structures appropriate for network analysis and give a brief description of fundamental algorithms.
The purpose of this section is not to list different algorithms and source code, but rather to sketch the basic ideas behind the data structures and algorithms.
For source code of data structures and algorithms, the reader is encouraged to the lecture of \cite{algorithm_design}.

\paragraph{Matrix implementation\color{Cayenne}{.}}
To begin with, we consider the implementation of adjacency matrices as introduced in section \ref{sec:network_matrices}.
Matrix implementations work also for the other matrices introduced in section \ref{sec:network_matrices}.
All adjacency matrices are by definition square matrices.
Their entries are either $0$ or $1$, and can take any floating number value for weighted networks.
In this work, we neglect negative edge weights.
The number of nodes in most complex network datasets is relatively large. 
Starting with small networks (100 nodes, conference contacts \cite{isella2011}), complex networks can be gigantic (0.5 billion nodes, twitter tweeds \cite{Yang:2011}).
Note that the size of the adjacency matrices scales with the square of the networks size, hence large networks intractable for straightforward computer-based matrix analyses.

Nevertheless, there is one feature, that most adjacency matrices of real-world networks have in common:
they are \emph{sparse}, i.e. the vast majority of their entries are zeros\footnote{Typically, the number of edges in the network is of the same order as the number of nodes.}.
Since zeros do not contribute to matrix operations as products or additions, it is reasonable to use data structures ignoring zeros.
These data structures are called \verb"sparse matrices".
Their advantages is (1) they save much memory and (2) computations are faster, because operations with zeros involved are not executed.
Sparse matrix data structures are available in most modern computer languages (e.g. Matlab, Python: \verb"scipy" library, C/C++: \verb"boost" library).
They perform well for all problems based on adjacency matrices, e.g. degree or eigenvector centrality.
However, matrix methods are not suitable for the computation of many other network measures, such as betweenness, closeness or network navigation.

\paragraph{Graph implementation\color{Cayenne}{.}}
The weakness of matrix representations of networks is that it is rather complicated to \emph{traverse} a network using matrices.
A traversal is a procedure like: start at a node, visit all of its neighbors, from each neighbor visit its neighbor and so forth, until there are no more new nodes to traverse.
This is a searching process.
These processes are used in many implementations of graph theoretic methods.

An alternative implementation to the adjacency matrix is an \emph{adjacency list}.
It stores the neighbors of every node and is implemented in terms of a linked list.
Using the example network of \ref{fig:simple_digraph}, we get the following adjacency list:
\begin{align*}
1 &\rightarrow 2,3 \\
2 &\rightarrow 4 \\
3 &\rightarrow 2 \\
4 &\rightarrow 2,3 .
\end{align*}
In order to traverse the graph starting at node $1$, we can choose any of the neighbors of $1$ and repeat the process until we have traversed all nodes.
One possible traversal starting at $1$ would be $1\rightarrow 3 \rightarrow 2 \rightarrow 4$.

During a traversal process, one can decide to either exploit the whole neighborhood of a node first and then traverse the next generation or choose a neighbor of every traversed node at every step.
These two essential searching processes are called breadth-first-search (BFS) and depth-first-search (DFS), respectively.
The difference between the two lies in the order of traversed nodes.
Figure \ref{fig:dfs_bfs} shows resulting search trees of the two methods.
Starting at node $1$, the traversal $1\rightarrow 3 \rightarrow 2 \rightarrow 4$ would be found using a DFS-search, while a BFS-search would yield $1\rightarrow 2 \rightarrow 3 \rightarrow 4$.
It should be noted that in general there exist multiple BFS and DFS trees for each starting node. 
%
\begin{figure}[htbp]
\begin{center}
\includegraphics{DFS-BFS.pdf}
\caption{Breadth-first-search and depth-first-search trees in the directed network of fig.~\ref{fig:simple_digraph}. Searches are started at node $1$.}
\label{fig:dfs_bfs}
\end{center}
\end{figure}

Both search algorithms are used in many applications.
BFS is efficient to compute shortest paths in unweighted networks.
With every generation in a BFS tree, the distance from the starting node is incremented by 1, and thus the set of nodes with a certain distance from the starting node can be directly read from the BFS tree (see figure \ref{fig:dfs_bfs}).
Shortest paths in weighted networks can be identified using a the algorithm of Dijkstra \cite{Dijkstra:1959}.
DFS can be used to identify connected components in directed graphs (see next section).

Implementations of graph structures as discussed above are for example available in the libraries \verb"networkx" (Python), \verb"igraph" (C, Python, R), \verb"Lemon" and \verb"Boost" (C++).

\paragraph{Hard problems\color{Cayenne}{.}}
Although the libraries introduced above provide a huge and efficient toolbox for network analysis, there are still network problems, where no efficient algorithm is known for their exact solution.
In the language of complexity theory, the time to solve these problems scales with the problem size in non-polynomial time.
Problems of this kind can typically be solved exactly only for small system sizes.

Probably the most popular example is the \emph{traveling salesman problem}: a salesman has to traverse a set of cities and thereby choose the order of those cities that minimizes the total distance.
For small problem sizes, it is possible just to try out all possible combinations and find the minimal total distance.
The number of possible combinations, however, grows factorial with the system size, i.e. finding a solution takes $t\propto n!$ for $n$ cities.
In other words, if the problem could be solved for 20 cities in 1 second, it would take 21 seconds to solve it for 21 cities, 7 minutes for 22 cities and 3 million years for 30 cities!

A more exhaustive overview about hard problems is in \cite{algorithm_design} and the references therein.
Generally, heuristic methods have to be used in order to get an approximate solution.
It should be noted that the \emph{maximum clique} problem (section xx) and the \emph{graph partitioning} (see section xx, \cite{brandes2007}) belong to the class of hard problems.


\section{Network models and epidemiology}\label{sec:network_models}
The analysis of real-world networks in terms of the measures introduced in section \ref{sec:network_theory} has given useful insight into the structural properties of these systems.
In particular, observations showed that many networks have heavy-tailed degree distributions and show non vanishing clustering coefficients.
In this section we summarize the results of some widely used network models.
At the end of the section, we give a comparison between the different models an discuss their relevance in epidemiology.

\subsection{Lattice model}
Lattice models are inherently related to homogeneously distributed geographical positions of individuals.
They show a high degree of regularity and their potential for SIS and SIR spreading processes has been studied in \cite{Harris:1974} and \cite{Bak:1990}, respectively.

\subsection{Erd\H{o}s-R\'enyi model}
The Erd\H{o}s-R\'enyi model makes use of probabilistic methods to analyze network properties and is therefore a random graph model.
A \emph{random network} is generated by generating a set of $N$ nodes and connect each of the $\frac{1}{2}N(N-1)$ possible node pairs\footnote{We focus on undirected networks here. In the directed case, there are $N(N-1)$ possible node pairs.} with a certain probability $p$.
Networks generated this way are often called $G_{N,p}$ networks, although they are actually elements of a $G_{N,p}$ \emph{ensemble}\footnote{An equivalent approach is to consider a fixed number of edges $m$ instead, yielding a $G_{N,m}$ ensemble.}.

Random graph theory addresses questions about typical properties of networks with $N\rightarrow \infty $ nodes.
Consequently, the edge occupation probability $p$ is the key parameter in random graph theory.
Properties of particular interest are the average shortest path length or the distributions of degrees, component sizes (percolation) and the occurrence of special subgraphs such as triangles.
Apparently, the expected number of edges in the network is $\mean{E}=\frac{1}{2}pN(N-1)$, if $p$ is the edge occupation probability.
In addition, every edge increases the degree of two vertices, so that the \emph{average degree} of a random network of $N$ nodes is
\begin{equation}\label{eq:mean_er_degree}
\mean{k}=\frac{2\mean{E}}{N}=(N-1)p\simeq pN .
\end{equation}
In the directed case, we would get the same result for both, in degree and out degree, since the factors $2$ and $\frac{1}{2}$ would just disappear in \eqref{eq:mean_er_degree}.
It should be noted that the mean degree is the most appropriate parameter for the analysis of random graphs.
Equation \eqref{eq:mean_er_degree} demonstrates that the system behavior for each value of $p$ depends on the system size.

We obtain the \emph{degree distribution} of $G_{N,p}$, if we realize that the probability to find a node with degree $k$ is equal to the probability to find a node that is connected to $k$ other nodes, but not to the $N-k-1$ remaining nodes in the network.
Thus, the degree distribution is given by a bimodal distribution
\begin{equation}\label{eq:bimodal}
P(k)= \left(\begin{array}{c}N-1 \\k\end{array}\right) p^k (1-p)^{N-k-1} .
\end{equation}
Provided that we are interest in large networks ($N\rightarrow \infty $), equation \eqref{eq:bimodal} is approximated by a Poisson distribution, 
\begin{equation}\label{eq:poisson_distribution}
P(k) =\frac{\mean{k}^k }{k!} e^{-\mean{k}}
\end{equation}
i.e. there is variation in the degrees, but there still remains a \emph{typical degree} in the system.

It is an interesting feature of random graphs that for different edge occupation probabilities they show different phases.
For low values of $p$, nodes tend to form small connected components, whereas for increasing $p$ suddenly a \emph{giant component} emerges.
This giant component contains almost all nodes of the network.
The behavior for large values of $p$ has first been studied by Erd\H{o}s and R\'enyi \cite{ER:1959}.
A few years later, Erd\H{o}s and R\'enyi found thresholds for the emergence of subgraphs and a giant connected component \cite{ER:1960,ER:1961}.
Results for the occurrence of different subgraphs are summarized in \cite{Albert:2002}.

The size of the giant component and the mean component size can be computed analytically for random networks.
Following Newman \cite{Newman2003}, we observe that the probability that a node is in the giant component is equivalent to the probability that none of its neighbors are part of the giant component.
This probability is given by $u^k$, if $u$ is the fraction of nodes that are not in the giant component, i.e. $u$ is the probability that a randomly chosen node is not in the giant component.
An expression for $u$ can be obtained by averaging $u^k$ over all degrees $k$.
The degree distribution is given by \eqref{eq:poisson_distribution}.
Hence, the fraction of nodes not in the giant component is
\[
u=e^{\mean{k}(u-1)}.
\]
The size of the giant component is $S=1-u$ and consequently
\begin{equation}\label{eq:giant_component}
S=1-e^{-\mean{k}S}.
\end{equation}
One can use similar arguments to obtain an expression for the mean cluster size \cite{Newman2003}
\begin{equation}\label{eq:mean_cluster_size}
\mean{s}=\frac{1}{1-\mean{k}+\mean{k}S}.
\end{equation}
%
The largest connected component phase transition is shown in figure \ref{fig:ER_lcc_emergence}.
\begin{SCfigure}
\includegraphics{ER_emergence_LCC.pdf}
\caption{Emergence of the largest connected component (LCC) in an \ER graph as it follows from \eqref{eq:giant_component}
The size of the of the largest component takes finite values for $\mean{k}>1$.
The mean cluster size is given by equation \eqref{eq:mean_cluster_size} and diverges at $\mean{k}=1$.
}
\label{fig:ER_lcc_emergence}
\end{SCfigure}
%

Since all edges in a random network are independent and identically distributed, the probability that a given node is part of a connected triple is $p^2$.
In analogy, the probability that a given node belongs to a closed triangle is $p^3$.
Consequently, the \emph{clustering coefficient}  \eqref{eq:clustering_coefficient} of a $G_{N,p}$ network is given by
\begin{equation}\label{eq:er_clustering}
C=\frac{p^3}{p^2}=p=\frac{\mean{k}}{N}.
\end{equation}
Equation \eqref{eq:er_clustering} shows that the clustering coefficient of random graphs vanishes in the limit of large networks.
We end this section by giving an approximation of the average shortest path distance in random graphs. 
Starting at some node in the network, the average number of nodes at distance 1 is given by the mean degree $\mean{k}$. 
Hence, the average number of neighbors at distance $d$ is $\mean{k}^d$.
In order to reach all $N$ nodes in the network, we need $r$ steps, where $r$ is determined by $\mean{k}^r\simeq N$.
Thus, $r$ approximates the diameter of the network.
Since we are only interested in the rough behavior of the average shortest path length $\mean{l}$, we approximate it by $r$ \cite{dynamical_processes} and obtain 
\begin{equation}\label{eq:er_av_shortest}
\mean{l}\simeq \frac{\log N}{\log \mean{k}}.
\end{equation}
The average degree remains constant for different network orders, so that equation \eqref{eq:er_av_shortest} demonstrates that the average shortest path length grows logarithmically with the number of nodes in \ER graphs.
This relation is found in many complex networks and is an indication for the small-world effect introduced in the next section.

\subsection{Watts-Strogatz model}\label{sec:watts_strogatz_model}
We have seen that random graphs can reproduce some important properties of real-world networks, particularly the existence of a giant component and small average shortest path length.
Nevertheless, equation \eqref{eq:er_clustering} demonstrates that the tendency to form connected triangles is absent in large scale \ER networks.
Observations show, however, that many real-world networks show this feature \cite{WassermanFaust,Newman2003,Milgram:1967}.
It is characteristic for social networks in particular to have a high degree of clustering and at the same time short-cuts allowing for small average shortest path distances.
In this sense they can be seen as an intermediate structure between lattices (high local order) and random graphs (small shortest path distances).
Therefore, Watts and Strogatz introduced the \emph{small-world model} in 1998 \cite{Watts:1998}.
We briefly summarize some of its main findings.

A Watts-Strogatz model interpolates between lattices and random networks by rewiring edges of a lattice.
We start with a regular ring lattice of $N$ nodes, where each node is connected to $m$ of its nearest neighbors on the lattice.
Then, each edge is rewired randomly with probability $p$.
Keeping $m$ constant from the beginning yields a scalable topology for different values of $p$.
The clustering coefficient $C$ and the average shortest path length $\mean{l}$ for different values of $p$ are shown in figure \ref{fig:watts_strogatz}.
Both values are normalized by their corresponding values in the initial lattice, i.e. $C/C_0$ and $\mean{l}/\mean{l}_0$ respectively.
%
\begin{SCfigure}
\includegraphics{Watts_strogatz.pdf}
\caption{Clustering coefficient and average shortest path length in the Watts-Strogatz model.
Both quantities are  normalized to the the corresponding value for $p=0$.
Results for networks with $N=1000$ nodes and $m=10$.
Every data point is the average of 1000 realizations.}
\label{fig:watts_strogatz}
\end{SCfigure}

The degree distribution collapses to a single peak for $p=0$.
In their paper about properties of small-world networks \cite{Barrat:2000fj}, Barrat and Weigt showed that the distribution converges to a Poisson distribution in the limit $p\rightarrow 1$ and found an analytical approximation for the clustering coefficient for different values of $p$.

There is no sharp criterion for a network to be called small-world network.
Instead, a network is called small-world network, if it shows a sufficiently large clustering coefficient \emph{and} a sufficiently low average shortest path length.
This is the intermediate region in figure \ref{fig:watts_strogatz}. 

\subsection{Barab\'asi-Albert model}
Besides the critical behavior in \ER networks and the small-word effect in Watts-Strogatz networks, observations of real networks showed that they possess heavy-tailed degree distributions.
A central question is, where such distributions originate from.
Therefore, Barab\'asi and Albert introduced a network model in order to mimic the evolution of the world wide web \cite{Barabasi99}.
The system under consideration is a network of websites (nodes) that are connected by hyperlinks (edges) and should not be confused with the physical network of internet routers.
The evolution of the www-network is reduced to two simple principles.
(1) new nodes are added to the system over time and (2) the new nodes have a higher probability to link to existing nodes of higher degree.
The second principle can be summarized as a rich-get-richer phenomenon, i.e. the more links you have the more you will get.
In network language, this mechanism is called \emph{preferential attachment}.
It can be seen as the network version of what is also known as Matthew-effect or cumulative advantage \cite{Merton:1968fh,price:1976}.

The preferential attachment model for growing networks is as follows:
Start with a small number $m_0$ of nodes and add a new node at every time step.
Connect the new node to $m<m_0$ existing nodes, each with probability $\Pi $.
The probability for an existing node $i$ to be connected with the new one depends on the degree of $i$, i.e. $\Pi (k_i)=k_i/\sum _j k_j$.
\begin{SCfigure}
\includegraphics{BA_graph.pdf}
\caption{Cumulative degree distribution of a \BA graph with $N=10^5$ nodes and $m_0=m=5$.
The dashed line shows a power-law $P(k)\propto k^{-2}$.}
\label{fig:BA_cdf}
\end{SCfigure}

Figure \ref{fig:BA_cdf} shows the degree distribution of a network generated this way.
We have to point out that it is generally more appropriate to plot the cumulative distribution of such distributions, because it is more robust against statistical fluctuations, particularly in the tail of the distribution \cite{Clauset:2009}.
As the figure shows, the distribution is well approximated by a power law of the form
\[
P(k)\propto k^{-\gamma }
\]
with $\gamma =2$ for the cumulative distribution and $\gamma = 3$ for the probability density function, respectively.

Barab\'asi and Albert could show analytically that the resulting network has a power-law degree distribution of the form
\begin{equation}\label{eq:BA_law}
P(k)=2m^2 k^{-3}.
\end{equation}
Although the slope $\gamma = 3$ does not match the power-law exponent of the world wide web ($\gamma = 2.1\pm 0.1$ \cite{Barabasi99}) the model explains the existence of a scale-free degree distribution.

As a conceptual model, the \BA model has a huge field of applications for theoretical questions.
Nevertheless, the power-law degree behavior is also reproduced by fitness models \cite{Bianconi:2001,Fortunato:2006} and copy models \cite{Kleinberg99theweb}.
Fitness models allow for higher flexibility in terms of the power-law exponent.
However, the range of possible exponents cannot take values in the interval $0<\gamma < 2$ \cite{all_scale_free_are_sparse}.

Besides the models discussed above, there are other network models, such as the configuration model or exponential network models. 
The \emph{configuration model} is a more sophisticated random graph model that allows for arbitrary degree distributions \cite{Newman:book,Newman:2001pa}.
Moreover, the degree sequence of a given network remains constant.
In the configuration model one can consider higher order statistics, such as degree correlations and the clustering coefficient.
\emph{Exponential random graphs} are related to the concept of the micro canonical ensemble in statistical mechanics \cite{Strauss:86}.
In this context, an \ER graph is just one realization of an ensemble of possible random graphs.
Exponential random graphs are an elegant way of treating networks, but their mathematical treatments appears intractable for many cases of interest \cite{Newman2003}.

\subsection{Robustness and epidemics on different network types}



\bibliographystyle{plain}
\bibliography{/Users/lentz/Documents/GitHub_locals/Thesis/bibliography.bib}
\end{document}
